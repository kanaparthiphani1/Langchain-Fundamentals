{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e2e24d5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "os.environ['OPENAI_API_KEY']=os.getenv(\"OPENAI_API_KEY\")\n",
    "## Langsmith Tracking\n",
    "os.environ[\"LANGCHAIN_API_KEY\"]=os.getenv(\"LANGCHAIN_API_KEY\")\n",
    "os.environ[\"LANGCHAIN_TRACING_V2\"]=\"true\"\n",
    "os.environ[\"LANGCHAIN_PROJECT\"]=os.getenv(\"LANGCHAIN_PROJECT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "297c70f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "USER_AGENT environment variable not set, consider setting it to identify your requests.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inApache Kafka Deep DiveTirthraj Mahajan17 min read·Jul 26, 2025--ListenSharePress enter or click to view image in full sizeKafka ArchitectureProject StructureWe are going to refer the Apache Kafka Mirror on Github: https://github.com/apache/kafkaPress enter or click to view image in full sizeModules OverviewLet us take a look at the starting point of our deep dive. We’ll look at core module first.What happens when you start a kafka brokerThe bin/kafka-server-start.sh script is a startup wrapper for launching a Kafka brokerWhen you run the following line of codebin/kafka-server-start.sh [-daemon] config/server.propertiesIt prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka serverLet’s take a look at the bin/kafka-server-start.sh scriptif [ $# -lt 1 ]; then    echo \"USAGE: $0 [-daemon] server.properties [--override property=value]*\"    exit 1fiChecks if any arguments are passed.Requires at least the path to server.properties.Optionally supports -daemon to run Kafka in the background.if [ \"x$KAFKA_HEAP_OPTS\" = \"x\" ]; then    export KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\"fiSets default JVM heap size to 1 GB if not configured. You can override this by setting KAFKA_HEAP_OPTS manually before running the script.EXTRA_ARGS=${EXTRA_ARGS-\\'-name kafkaServer -loggc\\'}Optional JVM args; gives the JVM process a name and enables GC logging.COMMAND=$1case $COMMAND in  -daemon)    EXTRA_ARGS=\"-daemon \"$EXTRA_ARGS    shift    ;;  *)    ;;esacIf the first argument is -daemon, it modifies EXTRA_ARGS to start Kafka in the background and shifts the args so the next one becomes the config file path.exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka \"$@\"This line actually starts the Kafka broker by calling:kafka-run-class.sh kafka.Kafka server.propertiesThat script launches the JVM and runs the main class kafka.Kafka with your provided config file.The kafka-run-class.sh script is a launcher utility used by Apache Kafka to run any Java class with the correct classpath and JVM options preconfigured. It\\'s essentially a wrapper that sets up the environment and then runs:java [JVM options] -cp [Classpath] [Java class name] [arguments]If -daemon is passed, it uses nohup to run the class in the background.Entrypoint to BrokerWe now know that the bin/kafka-server-start.sh essentially calls the main class kafka.Kafka with your provided config fileThe JVM entry point kafka.Kafka is actually a Scala object defined in the core module of the Apache Kafka source.File path : core/src/main/scala/kafka/Kafka.scalaThis is the class that kafka-run-class.sh ultimately invokes when you start a broker in JVM mode.Let’s look at the overview of the code:package kafkaobject Kafka extends Logging {  def getPropsFromArgs(args: Array[String]): Properties = {    // Validate input (--help, --version).    // Load server.properties into a Properties object    // Apply any --override foo=bar pairs to temporarily override values.  }  private def buildServer(props: Properties): Server = {    // KafkaConfig encapsulates all the broker settings.    val config = KafkaConfig.fromProps(props, doLog = false)    // KafkaRaftServer is the implementation of the broker using the     // new Raft-based metadata quorum (since Kafka 3.3+).    // Returns a Server interface that you can     // .startup(), .shutdown(), .awaitShutdown() on.    new KafkaRaftServer(config, Time.SYSTEM)  }  def main(args: Array[String]): Unit = {    try {      val serverProps = getPropsFromArgs(args)      val server      = buildServer(serverProps)        // optional signal handler (logs “Received TERM, shutting down…”)      if (!OperatingSystem.IS_WINDOWS && !Java.isIbmJdk)        new LoggingSignalHandler().register()        // JVM shutdown hook to call server.shutdown()      Exit.addShutdownHook(\"kafka-shutdown-hook\", () => { server.shutdown() })        // start the server, or exit(1) on error      try server.startup()      catch { case e: Throwable => fatal(\"…\", e); Exit.exit(1) }        // block until shutdown      server.awaitShutdown()    }    catch {      case e: Throwable =>        fatal(\"Exiting Kafka due to fatal exception\", e)        Exit.exit(1)    }    Exit.exit(0)  }}Argument parsing → serverProps.Build the actual Server instance.Signal handler: on Unix-style signals (SIGTERM, etc.) it logs a message.Shutdown hook: ensures that even if you kill the JVM (e.g. CTRL-C), server.shutdown() is called cleanly.startup(): launches listener threads, log manager, controller, etc.If it throws, we log fatal and exit with code 1.awaitShutdown(): main thread blocks here until the broker is asked to shut down.On clean exit, we call Exit.exit(0) to return status 0.Server TraitThe Server.scala file inside core/src/main/scala/server/Server.scala defines a Server trait along with an object Server in Scala, which is a singleton object that provides utility methods and values for managing Kafka server processes and metrics.The Server trait defines an interface for server-like behavior that includes the following methods:trait Server {  def startup(): Unit  def shutdown(): Unit  def awaitShutdown(): Unit}startup(): This is an abstract method that is supposed to be implemented by any class that mixes in the Server trait. This method will be responsible for starting up the server.shutdown(): This is another abstract method that any implementing class needs to define. It will handle shutting down the server.awaitShutdown(): This method will likely block and wait until the server has fully shut down.The Server object is a companion object to the Server trait. It provides utility methods, constants, and configurations related to the server but does not directly implement the server functionality (like starting, shutting down, etc.). Instead, it provides helper functions for metrics and configurations.Constants:val MetricsPrefix: String = \"kafka.server\"val ClusterIdLabel: String = \"kafka.cluster.id\"val NodeIdLabel: String = \"kafka.node.id\"These constants are used to help label or organize the metrics for Kafka in a more structured way.object Server {  def initializeMetrics(...){    // Initializes Kafka metrics for monitoring.  }  def buildMetricsConfig(...){    // Builds and returns a MetricConfig object,     // which configures how metrics will be collected.  }    // Defines different states that   // the server can be in during its lifecycle.  // This is useful for tracking the server\\'s status at any given point  sealed trait ProcessStatus  case object SHUTDOWN extends ProcessStatus  case object STARTING extends ProcessStatus  case object STARTED extends ProcessStatus  case object SHUTTING_DOWN extends ProcessStatus}ProcessStatusis a small ADT capturing the state of a server’s lifecycle.Used internally by implementations of Server to guard against illegal transitionsWe saw that in core/src/main/scala/kafka/Kafka.scala, the buildServer function creates an instance of KafkaRaftServer which implements the Server trait and provides concrete implementations of all three lifecycle methods.KafkaRaftServerYou can find the implementation of KafkaRaftServer in the file core/src/main/scala/server/KafkaRaftServer.scala/** * This class implements the KRaft (Kafka Raft) mode server which relies * on a KRaft quorum for maintaining cluster metadata. It is responsible for * constructing the controller and/or broker based on the `process.roles` * configuration and for managing their basic lifecycle (startup and shutdown). * */class KafkaRaftServer(  config: KafkaConfig,  time: Time,) extends Server with Logging {  // ...}All of the low-level plumbing (network listeners, Raft client, metadata log, fault handlers, etc.) lives in SharedServer. Both the controller and broker components will share this instance.private val sharedServer = new SharedServer(  config,  metaPropsEnsemble,  time,  metrics,  // parse quorum config…  CompletableFuture.completedFuture(QuorumConfig.parseVoterConnections(config.quorumConfig.voters)),  QuorumConfig.parseBootstrapServers(config.quorumConfig.bootstrapServers),  new StandardFaultHandlerFactory(),  ServerSocketFactory.INSTANCE,)Depending on your process.roles setting you’ll get:broker: handles the usual produce/fetch API, log segments, replication, etc.controller: manages cluster metadata (topics, partitions, config changes) via Raft.By splitting those two roles, KafkaRaftServer gives you the flexibility to run:Combined (controller + broker) on every node (default for small clusters),Or Dedicated controllers separate from brokers for large-scale deployments.private val broker: Option[BrokerServer] = if (config.processRoles.contains(ProcessRole.BrokerRole)) {    Some(new BrokerServer(sharedServer))  } else {    None  }private val controller: Option[ControllerServer] = if (config.processRoles.contains(ProcessRole.ControllerRole)) {    Some(new ControllerServer(      sharedServer,      KafkaRaftServer.configSchema,      bootstrapMetadata,    ))  } else {    None  }Both BrokerServer and ControllerServer share a SharedServer instance, which sets up the common networking, storage (metadata log + data log), Raft client, metrics registry, and fault-handling machinery.We’ll take a look into BrokerServer , ControllerServer and SharedServer after this sectionLifecyle methods:Startupoverride def startup(): Unit = {    Mx4jLoader.maybeLoad()    // Controller component must be started before the broker component so that    // the controller endpoints are passed to the KRaft manager    controller.foreach(_.startup())    broker.foreach(_.startup())    AppInfoParser.registerAppInfo(Server.MetricsPrefix, config.brokerId.toString, metrics, time.milliseconds())    info(KafkaBroker.STARTED_MESSAGE)  }Optionally load JMX extensions.Start controller before broker (so broker can discover controller endpoints).Start broker (if configured).Register JMX AppInfo (pid, start time).Log a “server started” banner.You start the controller first so that it can publish its endpoints into the Raft quorum; then the broker starts and immediately knows how to talk metadata (e.g., “Which partitions am I leader for?”).Shutdownoverride def shutdown(): Unit = {    // In combined mode, we want to shut down the broker first, since the controller may be    // needed for controlled shutdown. Additionally, the controller shutdown process currently    // stops the raft client early on, which would disrupt broker shutdown.    broker.foreach(_.shutdown())    controller.foreach(_.shutdown())    CoreUtils.swallow(AppInfoParser.unregisterAppInfo(Server.MetricsPrefix, config.brokerId.toString, metrics), this)  }Shut down broker (so it can consult the controller if needed).Then shut down controller.Unregister JMX info, swallowing any errors.You shut down the broker first, since it may need to call the controller for a controlled partition demotion; then the controller rolls back its Raft client and cleans up.SharedServerSharedServer is a component in Kafka’s KRaft mode. It\\'s responsible for managing infrastructure that is shared between the Broker and Controller roles when running Kafka in KRaft mode — especially when both roles are combined in a single process.It manages shared components:Raft manager (KafkaRaftManager)Metadata loader (MetadataLoader)Snapshot generator & emitterFault handlingMetrics (Broker and Controller)Let’s take a look at the code/** * The SharedServer manages the components which are shared between the BrokerServer and * ControllerServer. These shared components include the Raft manager, snapshot generator, * and metadata loader. A KRaft server running in combined mode as both a broker and a controller * will still contain only a single SharedServer instance. * * The SharedServer will be started as soon as either the broker or the controller needs it, * via the appropriate function (startForBroker or startForController). Similarly, it will be * stopped as soon as neither the broker nor the controller need it, via stopForBroker or * stopForController. One way of thinking about this is that both the broker and the controller * could hold a \"reference\" to this class, and we don\\'t truly stop it until both have dropped * their reference. We opted to use two booleans here rather than a reference count in order to * make debugging easier and reduce the chance of resource leaks. */class SharedServer(  private val sharedServerConfig: KafkaConfig,  val metaPropsEnsemble: MetaPropertiesEnsemble,  val time: Time,  private val _metrics: Metrics,  val controllerQuorumVotersFuture: CompletableFuture[JMap[Integer, InetSocketAddress]],  val bootstrapServers: JCollection[InetSocketAddress],  val faultHandlerFactory: FaultHandlerFactory,  val socketFactory: ServerSocketFactory) extends Logging {  // ...}It’s constructed early in the server lifecycle, and then started either by the broker or controller, or both.Here are the main components of the classPress enter or click to view image in full sizeThe internal start() method of Kafka’s SharedServersets up the KRaft metadata infrastructure. This is the heart of how Kafka works in KRaft modeHere’s a simplified pseudocode for starting the KafkaRaft// Synchronized — ensures only one thread initializes the shared server at a timeprivate void synchronized start() {  // The started flag ensures idempotence — only one actual startup.  if already started -> return  init metrics  // Initializes support for dynamic configs (e.g., configs that can be updated at runtime from metadata).  init dynamic config  // Then Kafka initializes metrics for each role:  if broker -> init broker metrics  if controller -> init controller metrics    // Wrap both metric sets into a single object, so that KafkaRaftManager can report metrics to either/both.  create KafkaRaftManager  // Start the KafkaRaftManager  start RaftManager  create MetadataLoader (with metrics, fault handler, etc)  create SnapshotEmitter + SnapshotGenerator  install snapshot generator into loader  register loader with RaftManager    // If everything succeeds, the server is marked as started.  mark started = true}KafkaRaftManager is a critical component and here’s why —Before Kafka 2.8, Kafka used Apache ZooKeeper to store and replicate metadata:Topic configsPartition assignmentsACLsBroker registrationController electionZooKeeper was a central point of failure and required extra management.Kafka uses its own Raft Consensus protocol to store metadata in the Kafka cluster itself. KRaft = Kafka + RaftWith KRaft, Kafka replaces Zookeeper by:Introducing a metadata quorum (a Raft cluster of controllers)Using Kafka’s own Raft implementation to replicate metadataEmbedding the Raft protocol directly into the broker/controller logicKafkaRaftManager is the core class that manages the Raft protocol for metadata.Understanding Kafka’s Raft ProtocolLet’s start from the top. When you set up a Kafka cluster, there are two types of server roles — Broker Servers and Controller ServersPress enter or click to view image in full sizeKafka nodes take on one or both of these rolesBroker ServersStore topic partitionsHandle produce/fetch requestsMust have at least one broker in every clusterIn combined mode, a node can be both broker and controllerController ServersManage cluster metadata:Topic/partition definitionsReplica assignmentsLeader electionsBroker heartbeatsMore importantly, they are the ones that participate in the Raft algorithm in Kafka’s KRaft. Raft is used only for cluster metadata, not for message replication (which still uses Kafka’s own replication protocol per partition)There can only be one active controller, which serves as the leader, and other controllers are candidates that replicate the metadata log using Raft.A topic is split into partitions. Each partition is replicated across multiple brokers. Replication happens per partition, not across partitions.Each partition has:1. One leader replica (handles reads/writes)2. One or more follower replicas (sync with the leader).Thus, all produce(write) requests must go to the partition leader.The active controller decides which broker should be the leader for each partition. It also decides:1. Where the partition replicas are located2. When a leader should be changed (e.g., if a broker fails)Replicas must live on different brokers. Kafka never places a replica of the same partition on the same broker. This ensures that losing one broker doesn’t cause total partition data loss. Replicas can be configured per topic using replication.factorFailure & Recovery in the Metadata LayerPress enter or click to view image in full sizeRaft Consensus Algorithm Overview — Tirthraj MahajanWhen the active controller dies:Detection: Followers miss heartbeats.Election: After a timeout (e.g., 150–300 ms), a new Raft election begins.Quorum Vote: A candidate with the most up-to-date log gains majority votes.New Leader: The elected node resumes metadata updates and may send snapshots to lagging peers.Because Raft ensures only nodes with complete logs can become leaders, metadata consistency is always maintained.After the leader is elected, followers replicate the leader’s log. Kafka uses snapshot transmission (compressed, efficient) to catch them up.So overall, Kafka in KRaft mode is fundamentally a hybrid architecture. Kafka uses Raft (via the KafkaRaftManager) to metadata management and leader election and it uses its own log-based replication model for partition data replication.Understanding Kafka BrokerLet us first look into the KafkaBroker trait which is defined in core/src/main/scala/kafka/server/KafkaBroker.scala This trait defines the interface and common fields for any broker implementationtrait KafkaBroker extends Logging {  // core pieces every broker must expose  def config: KafkaConfig  def brokerState: BrokerState  def clusterId: String  def metrics: Metrics  def socketServer: SocketServer  def logManager: LogManager  def replicaManager: ReplicaManager  def groupCoordinator: GroupCoordinator  def dataPlaneRequestHandlerPool: KafkaRequestHandlerPool  def dataPlaneRequestProcessor: KafkaApis  def startup(): Unit  def shutdown(timeout: Duration): Unit  def awaitShutdown(): Unit  // …plus dozens of other helpers (quotas, schedulers, auth, token cache, etc.)}The BrokerSever class is essentially Kafka’s “main” broker process orchestrating everything needed to reliably store and serve your topic data.The BrokerServer is defined in core/src/main/scala/server/BrokerServer.scala file as follows:/** * A Kafka broker that runs in KRaft (Kafka Raft) mode. */class BrokerServer(  val sharedServer: SharedServer) extends KafkaBroker {  // ...}The one thing every BrokerServer instance gets from its SharedServer is “all the metadata plumbing” (Raft, the metadata log, snapshots, loader, etc.) and the fully initialized metrics and clock so it can focus on just the data‐plane logic.The broker initially has the SHUTDOWN statevar status: ProcessStatus = SHUTDOWN/*We say ProcessStatus earlier in Server Object:sealed trait ProcessStatus  case object SHUTDOWN extends ProcessStatus  case object STARTING extends ProcessStatus  case object STARTED extends ProcessStatus  case object SHUTTING_DOWN extends ProcessStatus*/The broker uses ReentrantLock on the status variable for consistency when changing the statusval lock: ReentrantLock = new ReentrantLock()private def maybeChangeStatus(from: ProcessStatus, to: ProcessStatus): Boolean = {    lock.lock()    try {      if (status != from) return false      info(s\"Transition from $status to $to\")      status = to      if (to == SHUTTING_DOWN) {        isShuttingDown.set(true)      } else if (to == SHUTDOWN) {        isShuttingDown.set(false)        awaitShutdownCond.signalAll()      }    } finally {      lock.unlock()    }    true  }In the section on KafkaRaftServer, we saw the following code in startup() function:private val broker: Option[BrokerServer] = if (config.processRoles.contains(ProcessRole.BrokerRole)) {  Some(new BrokerServer(sharedServer))} else {  None}override def startup(): Unit = {    // ...    controller.foreach(_.startup())    broker.foreach(_.startup())    // ...  }Now that we have a better understanding, lets dive into the startup function of the brokerWhen you call BrokerServer.startup(), Kafka spins up a complex stack of components in a carefully-ordered sequence so that metadata comes up first (via the Raft controller), then the data plane (logs, replication, client requests) can safely begin.Here is an overview of the codefunction startup():  // 1. Transition state  // If the state is other than SHUTDOWN, don\\'t run the function  if status != SHUTDOWN:    return  // Transition from SHUTDOWN -> STARTING  status = STARTING  // 2. Bring up metadata subsystem (Raft, loader, snapshots)  sharedServer.startForBroker()    // This ensures cluster metadata is loaded before any data I/O  // 3. Initialize dynamic configs & quotas from metadata snapshot  initDynamicConfig()  quotaManagers = createQuotaManagers(config, metrics, time)  // 4. Start background scheduler for retention, snapshots, etc.  kafkaScheduler = new Scheduler(config.backgroundThreads)  kafkaScheduler.start()  // 5. Prepare disk-based log manager (no recovery yet)  metadataCache = new MetadataCache(nodeId, raftManager.version)  logManager     = new LogManager(config, metadataCache, kafkaScheduler)  // 6. Set up lifecycle manager & security (token cache, credentials)  lifecycleManager    = new BrokerLifecycleManager(config, logManager.directories)  tokenCache          = new DelegationTokenCache(...)  credentialProvider  = new CredentialProvider(...)  // 7. Wait for controller quorum to be known, then start metadata forwarding  await(sharedServer.controllerQuorumVotersFuture, startupDeadline)  controllerProvider  = new ControllerNodeProvider(raftManager, config)  channelMgr          = new ChannelManager(controllerProvider, metrics, time)  channelMgr.start()  forwardingManager   = new ForwardingManager(channelMgr)  // 8. Build network layer (bind ports, auth) but delay request processing  apiVersionMgr = new ApiVersionManager(forwardingManager, metadataCache, config)  socketServer  = new SocketServer(config, credentialProvider, apiVersionMgr)  // 9. Instantiate the core data-plane: ReplicaManager  replicaManager = new ReplicaManager(    config, metrics, time, kafkaScheduler,    logManager, /*...*/, metadataCache  )  // 10. Register this broker in metadata via publishers (registration, configs, ACLs…)  publishers = buildBrokerMetadataPublishers(...)  sharedServer.loader.installPublishers(publishers)  // 11. Block until caught up to metadata high-watermark  await(lifecycleManager.initialCatchUpFuture, startupDeadline)  // 12. Once caught up, “unfence” and start serving client I/O  await(lifecycleManager.setReadyToUnfence(), startupDeadline)  socketServer.enableRequestProcessing()  // now begins handling produce/fetch  // 13. Finally, mark broker as fully started  status = STARTEDWe’ll mainly focus on Step 8 and 9 as others are self-explanatory.In Step 8 : We instantiate the SocketServer, binding the configured listener ports (PLAINTEXT, SSL, SASL, etc.). Delay actual request processing until after all other setup completes.In Step 9 : We instantiate a ReplicaManager . The ReplicaManager is the heart of Kafka’s data‐plane on each broker. Its job is to make sure each partition replica on that broker:Stores and serves data,Keeps in sync with the leader (if this broker is a follower), andPerforms leader duties (if this broker is the leader)We’ll learn more about ReplicaManager when we talk about how Kafka handles the partition data.SocketServerIt manages network connections, request processing, and thread coordination.Press enter or click to view image in full sizeKafka Broker ArchitectureKafka follows separation of concern by having two types of threadsNetwork threads focus only on socket I/O (non‐blocking reads & writes).Handler threads focus only on request processing (which may block on disk or be compute‐intensive).Kafka’s network layer is built on Java NIO (non‐blocking sockets), but it combines that with a small pool of dedicated handler threads to maximize throughput and isolate I/O from request processing.Let us first look into Java NIO SocketsJAVA Non-Blocking I/O is widely used in socket programming where a client and server can bind to a port or a socket to exchange messages through a shared buffer.Java NIO is built around non-blocking Channels, Selectors, and Buffers.Non-blocking I/O: Java NIO is based on a non-blocking I/O model, which means that multiple I/O operations can be managed by a single thread. This allows you to handle multiple connections simultaneously without the need for a thread per connection, resulting in better scalability and reduced resource consumption.A Channel represents an open connection to an entity like a file or network socket.SocketChannel (for TCP client connections)ServerSocketChannel (for TCP server sockets)DatagramChannel (for UDP sockets)Channels can generally be used for both reading and writing. Unlike traditional InputStream/OutputStream, channels can be configured to operate in non-blocking mode.In blocking I/O (the traditional InputStream/OutputStream), if you try to read data and no data is available, the call will block (pause) until data arrives. In non-blocking I/O, if no data is available, the call will return immediately, usually with a return value like 0 or -1 (depending on the method), indicating that nothing could be done at the moment.Selectors: Java NIO introduces the concept of selectors, which allows a single thread to monitor multiple channels for I/O events. With selectors, you can efficiently manage multiple network connectionsIn short, a Selector multiplexes I/O — it lets one thread manage thousands of non-blocking sockets efficiently.Press enter or click to view image in full sizeYou can study in-depth about Java NIO Sockets : Click hereEach DataPlaneAcceptor creates N processor threads, each with its own Java NIO SelectorThose threads:Accept new SocketChannels on the ServerSocket (in a single acceptor thread).Register each channel in non‐blocking mode with one of their selectors.Select() on that selector for read (and later write) events.Read raw bytes off the socket into a buffer (non‐blocking) and build a request object.Enqueue the parsed request on the central RequestChannelKafka then hands off each parsed request to one of M handler threads. Those threads typically block on RequestChannel.receiveRequest(), do CPU work and disk interaction (log append, fetch data), then build a response object. The response is pushed back onto the RequestChannel so the original processor thread can pick it up and do the socket write.So when a response is ready, the processor registers the channel for write interest, and on the next select() it serializes the response out to the same TCP socket.But what would happen if a produce or fetch request can’t complete immediately? This is where Purgatory comes into picturePurgatoryIn Kafka’s architecture, Purgatory is the in‐memory data structure for delayed operations that can’t be completed immediately because they’re waiting on some external condition. Rather than busy‐polling (“spin‐wait”) or blocking threads, Kafka registers those operations in Purgatory and wakes them only when their condition is met.There are two primary use-cases where you want to delay a response:Produce Requests with acks=all (or min.insync.replicas > 1)You need to wait until the record is replicated to enough in‐sync replicas before sending the “ACK” back to the producer.2. Fetch Requests with minBytes and maxWaitMsIf the log hasn’t yet accumulated minBytes of data, you either wait up to maxWaitMs or until that much data arrives—whichever comes first.Without Purgatory, the handler threads would have to block or poll in a tight loop, wasting resources. Purgatory lets them register interest and be notified only when something changes.How It WorksEnqueueing a Delayed OperationA handler thread examines a request (produce or fetch).If its conditions aren’t met (e.g. not enough replicas, not enough bytes), it creates a DelayedOperation object and registers it with Purgatory under a key (e.g. partition ID).2. Purgatory’s Data StructuresInternally, Kafka maintains a map from keys → sets of delayed ops.There are separate purgatories for produce (ProducerPurgatory) and fetch (FetchPurgatory), but they share the same basic design.3. Triggering ConditionsReplica Ack Events: when a follower replicates an append, the broker’s log manager notifies ProducerPurgatory that “partition P now has N acks.”Log Append Events: when new messages land in the page cache for a partition, the broker notifies FetchPurgatory that “partition P now has new data.”4. Waking Delayed OpsOn each trigger, Purgatory looks up the set of delayed ops for that partition.It tests each one’s condition (enoughAcks() or enoughBytes()).Those whose conditions are now satisfied get re-submitted to the regular request queue, so a handler thread can pick them up and send a real response.5. TimeoutsEach delayed op also carries a deadline (e.g. maxWaitMs), so Purgatory will expire it and return a (possibly empty) response if the timeout elapses first.KafkaApache KafkaKafka InternalsScala----Written by Tirthraj Mahajan13 followers·7 followingNo responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n')]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Ingestion\n",
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1\")\n",
    "docs= loader.load()\n",
    "docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e26625",
   "metadata": {},
   "outputs": [],
   "source": [
    "## splitting\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "splitter = RecursiveCharacterTextSplitter(chunk_size=1000,chunk_overlap=200)\n",
    "split_docs = splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a6113e57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | MediumSitemapOpen in appSign upSign inMedium LogoWriteSign upSign inApache Kafka Deep DiveTirthraj Mahajan17 min read·Jul 26, 2025--ListenSharePress enter or click to view image in full sizeKafka ArchitectureProject StructureWe are going to refer the Apache Kafka Mirror on Github: https://github.com/apache/kafkaPress enter or click to view image in full sizeModules OverviewLet us take a look at the starting point of our deep dive. We’ll look at core module first.What happens when you start a kafka brokerThe bin/kafka-server-start.sh script is a startup wrapper for launching a Kafka brokerWhen you run the following line of codebin/kafka-server-start.sh [-daemon] config/server.propertiesIt prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka serverLet’s take a look at the bin/kafka-server-start.sh scriptif [ $# -lt 1 ]; then    echo'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka serverLet’s take a look at the bin/kafka-server-start.sh scriptif [ $# -lt 1 ]; then    echo \"USAGE: $0 [-daemon] server.properties [--override property=value]*\"    exit 1fiChecks if any arguments are passed.Requires at least the path to server.properties.Optionally supports -daemon to run Kafka in the background.if [ \"x$KAFKA_HEAP_OPTS\" = \"x\" ]; then    export KAFKA_HEAP_OPTS=\"-Xmx1G -Xms1G\"fiSets default JVM heap size to 1 GB if not configured. You can override this by setting KAFKA_HEAP_OPTS manually before running the script.EXTRA_ARGS=${EXTRA_ARGS-\\'-name kafkaServer -loggc\\'}Optional JVM args; gives the JVM process a name and enables GC logging.COMMAND=$1case $COMMAND in  -daemon)    EXTRA_ARGS=\"-daemon \"$EXTRA_ARGS    shift    ;;  *)    ;;esacIf the first argument is -daemon, it modifies EXTRA_ARGS to start Kafka in the background and shifts the args so the next one becomes the'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='EXTRA_ARGS=\"-daemon \"$EXTRA_ARGS    shift    ;;  *)    ;;esacIf the first argument is -daemon, it modifies EXTRA_ARGS to start Kafka in the background and shifts the args so the next one becomes the config file path.exec $base_dir/kafka-run-class.sh $EXTRA_ARGS kafka.Kafka \"$@\"This line actually starts the Kafka broker by calling:kafka-run-class.sh kafka.Kafka server.propertiesThat script launches the JVM and runs the main class kafka.Kafka with your provided config file.The kafka-run-class.sh script is a launcher utility used by Apache Kafka to run any Java class with the correct classpath and JVM options preconfigured. It\\'s essentially a wrapper that sets up the environment and then runs:java [JVM options] -cp [Classpath] [Java class name] [arguments]If -daemon is passed, it uses nohup to run the class in the background.Entrypoint to BrokerWe now know that the bin/kafka-server-start.sh essentially calls the main class kafka.Kafka with your provided config fileThe JVM entry point'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='to run the class in the background.Entrypoint to BrokerWe now know that the bin/kafka-server-start.sh essentially calls the main class kafka.Kafka with your provided config fileThe JVM entry point kafka.Kafka is actually a Scala object defined in the core module of the Apache Kafka source.File path : core/src/main/scala/kafka/Kafka.scalaThis is the class that kafka-run-class.sh ultimately invokes when you start a broker in JVM mode.Let’s look at the overview of the code:package kafkaobject Kafka extends Logging {  def getPropsFromArgs(args: Array[String]): Properties = {    // Validate input (--help, --version).    // Load server.properties into a Properties object    // Apply any --override foo=bar pairs to temporarily override values.  }  private def buildServer(props: Properties): Server = {    // KafkaConfig encapsulates all the broker settings.    val config = KafkaConfig.fromProps(props, doLog = false)    // KafkaRaftServer is the implementation of the broker using the     //'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='= {    // KafkaConfig encapsulates all the broker settings.    val config = KafkaConfig.fromProps(props, doLog = false)    // KafkaRaftServer is the implementation of the broker using the     // new Raft-based metadata quorum (since Kafka 3.3+).    // Returns a Server interface that you can     // .startup(), .shutdown(), .awaitShutdown() on.    new KafkaRaftServer(config, Time.SYSTEM)  }  def main(args: Array[String]): Unit = {    try {      val serverProps = getPropsFromArgs(args)      val server      = buildServer(serverProps)        // optional signal handler (logs “Received TERM, shutting down…”)      if (!OperatingSystem.IS_WINDOWS && !Java.isIbmJdk)        new LoggingSignalHandler().register()        // JVM shutdown hook to call server.shutdown()      Exit.addShutdownHook(\"kafka-shutdown-hook\", () => { server.shutdown() })        // start the server, or exit(1) on error      try server.startup()      catch { case e: Throwable => fatal(\"…\", e); Exit.exit(1) }        // block'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='() => { server.shutdown() })        // start the server, or exit(1) on error      try server.startup()      catch { case e: Throwable => fatal(\"…\", e); Exit.exit(1) }        // block until shutdown      server.awaitShutdown()    }    catch {      case e: Throwable =>        fatal(\"Exiting Kafka due to fatal exception\", e)        Exit.exit(1)    }    Exit.exit(0)  }}Argument parsing → serverProps.Build the actual Server instance.Signal handler: on Unix-style signals (SIGTERM, etc.) it logs a message.Shutdown hook: ensures that even if you kill the JVM (e.g. CTRL-C), server.shutdown() is called cleanly.startup(): launches listener threads, log manager, controller, etc.If it throws, we log fatal and exit with code 1.awaitShutdown(): main thread blocks here until the broker is asked to shut down.On clean exit, we call Exit.exit(0) to return status 0.Server TraitThe Server.scala file inside core/src/main/scala/server/Server.scala defines a Server trait along with an object Server in Scala,'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='clean exit, we call Exit.exit(0) to return status 0.Server TraitThe Server.scala file inside core/src/main/scala/server/Server.scala defines a Server trait along with an object Server in Scala, which is a singleton object that provides utility methods and values for managing Kafka server processes and metrics.The Server trait defines an interface for server-like behavior that includes the following methods:trait Server {  def startup(): Unit  def shutdown(): Unit  def awaitShutdown(): Unit}startup(): This is an abstract method that is supposed to be implemented by any class that mixes in the Server trait. This method will be responsible for starting up the server.shutdown(): This is another abstract method that any implementing class needs to define. It will handle shutting down the server.awaitShutdown(): This method will likely block and wait until the server has fully shut down.The Server object is a companion object to the Server trait. It provides utility methods, constants, and'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='This method will likely block and wait until the server has fully shut down.The Server object is a companion object to the Server trait. It provides utility methods, constants, and configurations related to the server but does not directly implement the server functionality (like starting, shutting down, etc.). Instead, it provides helper functions for metrics and configurations.Constants:val MetricsPrefix: String = \"kafka.server\"val ClusterIdLabel: String = \"kafka.cluster.id\"val NodeIdLabel: String = \"kafka.node.id\"These constants are used to help label or organize the metrics for Kafka in a more structured way.object Server {  def initializeMetrics(...){    // Initializes Kafka metrics for monitoring.  }  def buildMetricsConfig(...){    // Builds and returns a MetricConfig object,     // which configures how metrics will be collected.  }    // Defines different states that   // the server can be in during its lifecycle.  // This is useful for tracking the server\\'s status at any'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content=\"// which configures how metrics will be collected.  }    // Defines different states that   // the server can be in during its lifecycle.  // This is useful for tracking the server's status at any given point  sealed trait ProcessStatus  case object SHUTDOWN extends ProcessStatus  case object STARTING extends ProcessStatus  case object STARTED extends ProcessStatus  case object SHUTTING_DOWN extends ProcessStatus}ProcessStatusis a small ADT capturing the state of a server’s lifecycle.Used internally by implementations of Server to guard against illegal transitionsWe saw that in core/src/main/scala/kafka/Kafka.scala, the buildServer function creates an instance of KafkaRaftServer which implements the Server trait and provides concrete implementations of all three lifecycle methods.KafkaRaftServerYou can find the implementation of KafkaRaftServer in the file core/src/main/scala/server/KafkaRaftServer.scala/** * This class implements the KRaft (Kafka Raft) mode server which relies *\"),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='can find the implementation of KafkaRaftServer in the file core/src/main/scala/server/KafkaRaftServer.scala/** * This class implements the KRaft (Kafka Raft) mode server which relies * on a KRaft quorum for maintaining cluster metadata. It is responsible for * constructing the controller and/or broker based on the `process.roles` * configuration and for managing their basic lifecycle (startup and shutdown). * */class KafkaRaftServer(  config: KafkaConfig,  time: Time,) extends Server with Logging {  // ...}All of the low-level plumbing (network listeners, Raft client, metadata log, fault handlers, etc.) lives in SharedServer. Both the controller and broker components will share this instance.private val sharedServer = new SharedServer(  config,  metaPropsEnsemble,  time,  metrics,  // parse quorum config…  CompletableFuture.completedFuture(QuorumConfig.parseVoterConnections(config.quorumConfig.voters)),  QuorumConfig.parseBootstrapServers(config.quorumConfig.bootstrapServers),  new'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='quorum config…  CompletableFuture.completedFuture(QuorumConfig.parseVoterConnections(config.quorumConfig.voters)),  QuorumConfig.parseBootstrapServers(config.quorumConfig.bootstrapServers),  new StandardFaultHandlerFactory(),  ServerSocketFactory.INSTANCE,)Depending on your process.roles setting you’ll get:broker: handles the usual produce/fetch API, log segments, replication, etc.controller: manages cluster metadata (topics, partitions, config changes) via Raft.By splitting those two roles, KafkaRaftServer gives you the flexibility to run:Combined (controller + broker) on every node (default for small clusters),Or Dedicated controllers separate from brokers for large-scale deployments.private val broker: Option[BrokerServer] = if (config.processRoles.contains(ProcessRole.BrokerRole)) {    Some(new BrokerServer(sharedServer))  } else {    None  }private val controller: Option[ControllerServer] = if (config.processRoles.contains(ProcessRole.ControllerRole)) {    Some(new'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='{    Some(new BrokerServer(sharedServer))  } else {    None  }private val controller: Option[ControllerServer] = if (config.processRoles.contains(ProcessRole.ControllerRole)) {    Some(new ControllerServer(      sharedServer,      KafkaRaftServer.configSchema,      bootstrapMetadata,    ))  } else {    None  }Both BrokerServer and ControllerServer share a SharedServer instance, which sets up the common networking, storage (metadata log + data log), Raft client, metrics registry, and fault-handling machinery.We’ll take a look into BrokerServer , ControllerServer and SharedServer after this sectionLifecyle methods:Startupoverride def startup(): Unit = {    Mx4jLoader.maybeLoad()    // Controller component must be started before the broker component so that    // the controller endpoints are passed to the KRaft manager    controller.foreach(_.startup())    broker.foreach(_.startup())    AppInfoParser.registerAppInfo(Server.MetricsPrefix, config.brokerId.toString, metrics,'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='endpoints are passed to the KRaft manager    controller.foreach(_.startup())    broker.foreach(_.startup())    AppInfoParser.registerAppInfo(Server.MetricsPrefix, config.brokerId.toString, metrics, time.milliseconds())    info(KafkaBroker.STARTED_MESSAGE)  }Optionally load JMX extensions.Start controller before broker (so broker can discover controller endpoints).Start broker (if configured).Register JMX AppInfo (pid, start time).Log a “server started” banner.You start the controller first so that it can publish its endpoints into the Raft quorum; then the broker starts and immediately knows how to talk metadata (e.g., “Which partitions am I leader for?”).Shutdownoverride def shutdown(): Unit = {    // In combined mode, we want to shut down the broker first, since the controller may be    // needed for controlled shutdown. Additionally, the controller shutdown process currently    // stops the raft client early on, which would disrupt broker shutdown.    broker.foreach(_.shutdown())'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content=\"// needed for controlled shutdown. Additionally, the controller shutdown process currently    // stops the raft client early on, which would disrupt broker shutdown.    broker.foreach(_.shutdown())    controller.foreach(_.shutdown())    CoreUtils.swallow(AppInfoParser.unregisterAppInfo(Server.MetricsPrefix, config.brokerId.toString, metrics), this)  }Shut down broker (so it can consult the controller if needed).Then shut down controller.Unregister JMX info, swallowing any errors.You shut down the broker first, since it may need to call the controller for a controlled partition demotion; then the controller rolls back its Raft client and cleans up.SharedServerSharedServer is a component in Kafka’s KRaft mode. It's responsible for managing infrastructure that is shared between the Broker and Controller roles when running Kafka in KRaft mode — especially when both roles are combined in a single process.It manages shared components:Raft manager (KafkaRaftManager)Metadata loader\"),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='Broker and Controller roles when running Kafka in KRaft mode — especially when both roles are combined in a single process.It manages shared components:Raft manager (KafkaRaftManager)Metadata loader (MetadataLoader)Snapshot generator & emitterFault handlingMetrics (Broker and Controller)Let’s take a look at the code/** * The SharedServer manages the components which are shared between the BrokerServer and * ControllerServer. These shared components include the Raft manager, snapshot generator, * and metadata loader. A KRaft server running in combined mode as both a broker and a controller * will still contain only a single SharedServer instance. * * The SharedServer will be started as soon as either the broker or the controller needs it, * via the appropriate function (startForBroker or startForController). Similarly, it will be * stopped as soon as neither the broker nor the controller need it, via stopForBroker or * stopForController. One way of thinking about this is that both the'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='Similarly, it will be * stopped as soon as neither the broker nor the controller need it, via stopForBroker or * stopForController. One way of thinking about this is that both the broker and the controller * could hold a \"reference\" to this class, and we don\\'t truly stop it until both have dropped * their reference. We opted to use two booleans here rather than a reference count in order to * make debugging easier and reduce the chance of resource leaks. */class SharedServer(  private val sharedServerConfig: KafkaConfig,  val metaPropsEnsemble: MetaPropertiesEnsemble,  val time: Time,  private val _metrics: Metrics,  val controllerQuorumVotersFuture: CompletableFuture[JMap[Integer, InetSocketAddress]],  val bootstrapServers: JCollection[InetSocketAddress],  val faultHandlerFactory: FaultHandlerFactory,  val socketFactory: ServerSocketFactory) extends Logging {  // ...}It’s constructed early in the server lifecycle, and then started either by the broker or controller, or both.Here are'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='val socketFactory: ServerSocketFactory) extends Logging {  // ...}It’s constructed early in the server lifecycle, and then started either by the broker or controller, or both.Here are the main components of the classPress enter or click to view image in full sizeThe internal start() method of Kafka’s SharedServersets up the KRaft metadata infrastructure. This is the heart of how Kafka works in KRaft modeHere’s a simplified pseudocode for starting the KafkaRaft// Synchronized — ensures only one thread initializes the shared server at a timeprivate void synchronized start() {  // The started flag ensures idempotence — only one actual startup.  if already started -> return  init metrics  // Initializes support for dynamic configs (e.g., configs that can be updated at runtime from metadata).  init dynamic config  // Then Kafka initializes metrics for each role:  if broker -> init broker metrics  if controller -> init controller metrics    // Wrap both metric sets into a single object, so'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='init dynamic config  // Then Kafka initializes metrics for each role:  if broker -> init broker metrics  if controller -> init controller metrics    // Wrap both metric sets into a single object, so that KafkaRaftManager can report metrics to either/both.  create KafkaRaftManager  // Start the KafkaRaftManager  start RaftManager  create MetadataLoader (with metrics, fault handler, etc)  create SnapshotEmitter + SnapshotGenerator  install snapshot generator into loader  register loader with RaftManager    // If everything succeeds, the server is marked as started.  mark started = true}KafkaRaftManager is a critical component and here’s why —Before Kafka 2.8, Kafka used Apache ZooKeeper to store and replicate metadata:Topic configsPartition assignmentsACLsBroker registrationController electionZooKeeper was a central point of failure and required extra management.Kafka uses its own Raft Consensus protocol to store metadata in the Kafka cluster itself. KRaft = Kafka + RaftWith KRaft,'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='was a central point of failure and required extra management.Kafka uses its own Raft Consensus protocol to store metadata in the Kafka cluster itself. KRaft = Kafka + RaftWith KRaft, Kafka replaces Zookeeper by:Introducing a metadata quorum (a Raft cluster of controllers)Using Kafka’s own Raft implementation to replicate metadataEmbedding the Raft protocol directly into the broker/controller logicKafkaRaftManager is the core class that manages the Raft protocol for metadata.Understanding Kafka’s Raft ProtocolLet’s start from the top. When you set up a Kafka cluster, there are two types of server roles — Broker Servers and Controller ServersPress enter or click to view image in full sizeKafka nodes take on one or both of these rolesBroker ServersStore topic partitionsHandle produce/fetch requestsMust have at least one broker in every clusterIn combined mode, a node can be both broker and controllerController ServersManage cluster metadata:Topic/partition definitionsReplica'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='requestsMust have at least one broker in every clusterIn combined mode, a node can be both broker and controllerController ServersManage cluster metadata:Topic/partition definitionsReplica assignmentsLeader electionsBroker heartbeatsMore importantly, they are the ones that participate in the Raft algorithm in Kafka’s KRaft. Raft is used only for cluster metadata, not for message replication (which still uses Kafka’s own replication protocol per partition)There can only be one active controller, which serves as the leader, and other controllers are candidates that replicate the metadata log using Raft.A topic is split into partitions. Each partition is replicated across multiple brokers. Replication happens per partition, not across partitions.Each partition has:1. One leader replica (handles reads/writes)2. One or more follower replicas (sync with the leader).Thus, all produce(write) requests must go to the partition leader.The active controller decides which broker should be the'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='reads/writes)2. One or more follower replicas (sync with the leader).Thus, all produce(write) requests must go to the partition leader.The active controller decides which broker should be the leader for each partition. It also decides:1. Where the partition replicas are located2. When a leader should be changed (e.g., if a broker fails)Replicas must live on different brokers. Kafka never places a replica of the same partition on the same broker. This ensures that losing one broker doesn’t cause total partition data loss. Replicas can be configured per topic using replication.factorFailure & Recovery in the Metadata LayerPress enter or click to view image in full sizeRaft Consensus Algorithm Overview — Tirthraj MahajanWhen the active controller dies:Detection: Followers miss heartbeats.Election: After a timeout (e.g., 150–300 ms), a new Raft election begins.Quorum Vote: A candidate with the most up-to-date log gains majority votes.New Leader: The elected node resumes metadata updates'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='After a timeout (e.g., 150–300 ms), a new Raft election begins.Quorum Vote: A candidate with the most up-to-date log gains majority votes.New Leader: The elected node resumes metadata updates and may send snapshots to lagging peers.Because Raft ensures only nodes with complete logs can become leaders, metadata consistency is always maintained.After the leader is elected, followers replicate the leader’s log. Kafka uses snapshot transmission (compressed, efficient) to catch them up.So overall, Kafka in KRaft mode is fundamentally a hybrid architecture. Kafka uses Raft (via the KafkaRaftManager) to metadata management and leader election and it uses its own log-based replication model for partition data replication.Understanding Kafka BrokerLet us first look into the KafkaBroker trait which is defined in core/src/main/scala/kafka/server/KafkaBroker.scala This trait defines the interface and common fields for any broker implementationtrait KafkaBroker extends Logging {  // core pieces'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='is defined in core/src/main/scala/kafka/server/KafkaBroker.scala This trait defines the interface and common fields for any broker implementationtrait KafkaBroker extends Logging {  // core pieces every broker must expose  def config: KafkaConfig  def brokerState: BrokerState  def clusterId: String  def metrics: Metrics  def socketServer: SocketServer  def logManager: LogManager  def replicaManager: ReplicaManager  def groupCoordinator: GroupCoordinator  def dataPlaneRequestHandlerPool: KafkaRequestHandlerPool  def dataPlaneRequestProcessor: KafkaApis  def startup(): Unit  def shutdown(timeout: Duration): Unit  def awaitShutdown(): Unit  // …plus dozens of other helpers (quotas, schedulers, auth, token cache, etc.)}The BrokerSever class is essentially Kafka’s “main” broker process orchestrating everything needed to reliably store and serve your topic data.The BrokerServer is defined in core/src/main/scala/server/BrokerServer.scala file as follows:/** * A Kafka broker that runs in'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='everything needed to reliably store and serve your topic data.The BrokerServer is defined in core/src/main/scala/server/BrokerServer.scala file as follows:/** * A Kafka broker that runs in KRaft (Kafka Raft) mode. */class BrokerServer(  val sharedServer: SharedServer) extends KafkaBroker {  // ...}The one thing every BrokerServer instance gets from its SharedServer is “all the metadata plumbing” (Raft, the metadata log, snapshots, loader, etc.) and the fully initialized metrics and clock so it can focus on just the data‐plane logic.The broker initially has the SHUTDOWN statevar status: ProcessStatus = SHUTDOWN/*We say ProcessStatus earlier in Server Object:sealed trait ProcessStatus  case object SHUTDOWN extends ProcessStatus  case object STARTING extends ProcessStatus  case object STARTED extends ProcessStatus  case object SHUTTING_DOWN extends ProcessStatus*/The broker uses ReentrantLock on the status variable for consistency when changing the statusval lock: ReentrantLock = new'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='extends ProcessStatus  case object SHUTTING_DOWN extends ProcessStatus*/The broker uses ReentrantLock on the status variable for consistency when changing the statusval lock: ReentrantLock = new ReentrantLock()private def maybeChangeStatus(from: ProcessStatus, to: ProcessStatus): Boolean = {    lock.lock()    try {      if (status != from) return false      info(s\"Transition from $status to $to\")      status = to      if (to == SHUTTING_DOWN) {        isShuttingDown.set(true)      } else if (to == SHUTDOWN) {        isShuttingDown.set(false)        awaitShutdownCond.signalAll()      }    } finally {      lock.unlock()    }    true  }In the section on KafkaRaftServer, we saw the following code in startup() function:private val broker: Option[BrokerServer] = if (config.processRoles.contains(ProcessRole.BrokerRole)) {  Some(new BrokerServer(sharedServer))} else {  None}override def startup(): Unit = {    // ...    controller.foreach(_.startup())    broker.foreach(_.startup())    // ...'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content=\"{  Some(new BrokerServer(sharedServer))} else {  None}override def startup(): Unit = {    // ...    controller.foreach(_.startup())    broker.foreach(_.startup())    // ...  }Now that we have a better understanding, lets dive into the startup function of the brokerWhen you call BrokerServer.startup(), Kafka spins up a complex stack of components in a carefully-ordered sequence so that metadata comes up first (via the Raft controller), then the data plane (logs, replication, client requests) can safely begin.Here is an overview of the codefunction startup():  // 1. Transition state  // If the state is other than SHUTDOWN, don't run the function  if status != SHUTDOWN:    return  // Transition from SHUTDOWN -> STARTING  status = STARTING  // 2. Bring up metadata subsystem (Raft, loader, snapshots)  sharedServer.startForBroker()    // This ensures cluster metadata is loaded before any data I/O  // 3. Initialize dynamic configs & quotas from metadata snapshot  initDynamicConfig()\"),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='snapshots)  sharedServer.startForBroker()    // This ensures cluster metadata is loaded before any data I/O  // 3. Initialize dynamic configs & quotas from metadata snapshot  initDynamicConfig()  quotaManagers = createQuotaManagers(config, metrics, time)  // 4. Start background scheduler for retention, snapshots, etc.  kafkaScheduler = new Scheduler(config.backgroundThreads)  kafkaScheduler.start()  // 5. Prepare disk-based log manager (no recovery yet)  metadataCache = new MetadataCache(nodeId, raftManager.version)  logManager     = new LogManager(config, metadataCache, kafkaScheduler)  // 6. Set up lifecycle manager & security (token cache, credentials)  lifecycleManager    = new BrokerLifecycleManager(config, logManager.directories)  tokenCache          = new DelegationTokenCache(...)  credentialProvider  = new CredentialProvider(...)  // 7. Wait for controller quorum to be known, then start metadata forwarding  await(sharedServer.controllerQuorumVotersFuture, startupDeadline)'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='credentialProvider  = new CredentialProvider(...)  // 7. Wait for controller quorum to be known, then start metadata forwarding  await(sharedServer.controllerQuorumVotersFuture, startupDeadline)  controllerProvider  = new ControllerNodeProvider(raftManager, config)  channelMgr          = new ChannelManager(controllerProvider, metrics, time)  channelMgr.start()  forwardingManager   = new ForwardingManager(channelMgr)  // 8. Build network layer (bind ports, auth) but delay request processing  apiVersionMgr = new ApiVersionManager(forwardingManager, metadataCache, config)  socketServer  = new SocketServer(config, credentialProvider, apiVersionMgr)  // 9. Instantiate the core data-plane: ReplicaManager  replicaManager = new ReplicaManager(    config, metrics, time, kafkaScheduler,    logManager, /*...*/, metadataCache  )  // 10. Register this broker in metadata via publishers (registration, configs, ACLs…)  publishers = buildBrokerMetadataPublishers(...)'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='time, kafkaScheduler,    logManager, /*...*/, metadataCache  )  // 10. Register this broker in metadata via publishers (registration, configs, ACLs…)  publishers = buildBrokerMetadataPublishers(...)  sharedServer.loader.installPublishers(publishers)  // 11. Block until caught up to metadata high-watermark  await(lifecycleManager.initialCatchUpFuture, startupDeadline)  // 12. Once caught up, “unfence” and start serving client I/O  await(lifecycleManager.setReadyToUnfence(), startupDeadline)  socketServer.enableRequestProcessing()  // now begins handling produce/fetch  // 13. Finally, mark broker as fully started  status = STARTEDWe’ll mainly focus on Step 8 and 9 as others are self-explanatory.In Step 8 : We instantiate the SocketServer, binding the configured listener ports (PLAINTEXT, SSL, SASL, etc.). Delay actual request processing until after all other setup completes.In Step 9 : We instantiate a ReplicaManager . The ReplicaManager is the heart of Kafka’s data‐plane on each'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='SSL, SASL, etc.). Delay actual request processing until after all other setup completes.In Step 9 : We instantiate a ReplicaManager . The ReplicaManager is the heart of Kafka’s data‐plane on each broker. Its job is to make sure each partition replica on that broker:Stores and serves data,Keeps in sync with the leader (if this broker is a follower), andPerforms leader duties (if this broker is the leader)We’ll learn more about ReplicaManager when we talk about how Kafka handles the partition data.SocketServerIt manages network connections, request processing, and thread coordination.Press enter or click to view image in full sizeKafka Broker ArchitectureKafka follows separation of concern by having two types of threadsNetwork threads focus only on socket I/O (non‐blocking reads & writes).Handler threads focus only on request processing (which may block on disk or be compute‐intensive).Kafka’s network layer is built on Java NIO (non‐blocking sockets), but it combines that with a small'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='threads focus only on request processing (which may block on disk or be compute‐intensive).Kafka’s network layer is built on Java NIO (non‐blocking sockets), but it combines that with a small pool of dedicated handler threads to maximize throughput and isolate I/O from request processing.Let us first look into Java NIO SocketsJAVA Non-Blocking I/O is widely used in socket programming where a client and server can bind to a port or a socket to exchange messages through a shared buffer.Java NIO is built around non-blocking Channels, Selectors, and Buffers.Non-blocking I/O: Java NIO is based on a non-blocking I/O model, which means that multiple I/O operations can be managed by a single thread. This allows you to handle multiple connections simultaneously without the need for a thread per connection, resulting in better scalability and reduced resource consumption.A Channel represents an open connection to an entity like a file or network socket.SocketChannel (for TCP client'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='per connection, resulting in better scalability and reduced resource consumption.A Channel represents an open connection to an entity like a file or network socket.SocketChannel (for TCP client connections)ServerSocketChannel (for TCP server sockets)DatagramChannel (for UDP sockets)Channels can generally be used for both reading and writing. Unlike traditional InputStream/OutputStream, channels can be configured to operate in non-blocking mode.In blocking I/O (the traditional InputStream/OutputStream), if you try to read data and no data is available, the call will block (pause) until data arrives. In non-blocking I/O, if no data is available, the call will return immediately, usually with a return value like 0 or -1 (depending on the method), indicating that nothing could be done at the moment.Selectors: Java NIO introduces the concept of selectors, which allows a single thread to monitor multiple channels for I/O events. With selectors, you can efficiently manage multiple network'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='moment.Selectors: Java NIO introduces the concept of selectors, which allows a single thread to monitor multiple channels for I/O events. With selectors, you can efficiently manage multiple network connectionsIn short, a Selector multiplexes I/O — it lets one thread manage thousands of non-blocking sockets efficiently.Press enter or click to view image in full sizeYou can study in-depth about Java NIO Sockets : Click hereEach DataPlaneAcceptor creates N processor threads, each with its own Java NIO SelectorThose threads:Accept new SocketChannels on the ServerSocket (in a single acceptor thread).Register each channel in non‐blocking mode with one of their selectors.Select() on that selector for read (and later write) events.Read raw bytes off the socket into a buffer (non‐blocking) and build a request object.Enqueue the parsed request on the central RequestChannelKafka then hands off each parsed request to one of M handler threads. Those threads typically block on'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='and build a request object.Enqueue the parsed request on the central RequestChannelKafka then hands off each parsed request to one of M handler threads. Those threads typically block on RequestChannel.receiveRequest(), do CPU work and disk interaction (log append, fetch data), then build a response object. The response is pushed back onto the RequestChannel so the original processor thread can pick it up and do the socket write.So when a response is ready, the processor registers the channel for write interest, and on the next select() it serializes the response out to the same TCP socket.But what would happen if a produce or fetch request can’t complete immediately? This is where Purgatory comes into picturePurgatoryIn Kafka’s architecture, Purgatory is the in‐memory data structure for delayed operations that can’t be completed immediately because they’re waiting on some external condition. Rather than busy‐polling (“spin‐wait”) or blocking threads, Kafka registers those operations'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='delayed operations that can’t be completed immediately because they’re waiting on some external condition. Rather than busy‐polling (“spin‐wait”) or blocking threads, Kafka registers those operations in Purgatory and wakes them only when their condition is met.There are two primary use-cases where you want to delay a response:Produce Requests with acks=all (or min.insync.replicas > 1)You need to wait until the record is replicated to enough in‐sync replicas before sending the “ACK” back to the producer.2. Fetch Requests with minBytes and maxWaitMsIf the log hasn’t yet accumulated minBytes of data, you either wait up to maxWaitMs or until that much data arrives—whichever comes first.Without Purgatory, the handler threads would have to block or poll in a tight loop, wasting resources. Purgatory lets them register interest and be notified only when something changes.How It WorksEnqueueing a Delayed OperationA handler thread examines a request (produce or fetch).If its conditions aren’t'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='lets them register interest and be notified only when something changes.How It WorksEnqueueing a Delayed OperationA handler thread examines a request (produce or fetch).If its conditions aren’t met (e.g. not enough replicas, not enough bytes), it creates a DelayedOperation object and registers it with Purgatory under a key (e.g. partition ID).2. Purgatory’s Data StructuresInternally, Kafka maintains a map from keys → sets of delayed ops.There are separate purgatories for produce (ProducerPurgatory) and fetch (FetchPurgatory), but they share the same basic design.3. Triggering ConditionsReplica Ack Events: when a follower replicates an append, the broker’s log manager notifies ProducerPurgatory that “partition P now has N acks.”Log Append Events: when new messages land in the page cache for a partition, the broker notifies FetchPurgatory that “partition P now has new data.”4. Waking Delayed OpsOn each trigger, Purgatory looks up the set of delayed ops for that partition.It tests each'),\n",
       " Document(metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='a partition, the broker notifies FetchPurgatory that “partition P now has new data.”4. Waking Delayed OpsOn each trigger, Purgatory looks up the set of delayed ops for that partition.It tests each one’s condition (enoughAcks() or enoughBytes()).Those whose conditions are now satisfied get re-submitted to the regular request queue, so a handler thread can pick them up and send a real response.5. TimeoutsEach delayed op also carries a deadline (e.g. maxWaitMs), so Purgatory will expire it and return a (possibly empty) response if the timeout elapses first.KafkaApache KafkaKafka InternalsScala----Written by Tirthraj Mahajan13 followers·7 followingNo responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech')]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "split_docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "adebc7cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "## embeddings\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "embeddings= OpenAIEmbeddings()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c737b8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "vectordb = FAISS.from_documents(split_docs,embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2013871c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<langchain_community.vectorstores.faiss.FAISS at 0x2081f327f70>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectordb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d126ba08",
   "metadata": {},
   "source": [
    "Document Chain And Retrieval Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "510786d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "llm=ChatOpenAI(model=\"gpt-4o\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5fd572f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableLambda(format_docs)\n",
       "}), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "| ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n'), additional_kwargs={})])\n",
       "| ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002081F664A60>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002081F665480>, root_client=<openai.OpenAI object at 0x000002081F664AF0>, root_async_client=<openai.AsyncOpenAI object at 0x000002081F664B20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "| StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#document chain\n",
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"\n",
    "Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "\"\"\"\n",
    ")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm,prompt)\n",
    "document_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1bee1448",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieval chain\n",
    "retriever = vectordb.as_retriever()\n",
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever_chain=create_retrieval_chain(retriever,document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "667edbd6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'In Kafka\\'s architecture, what happens if a produce or fetch request can’t complete immediately?\\n\\nIn Kafka\\'s architecture, if a produce or fetch request can\\'t complete immediately because it is waiting for some external condition (e.g., enough replicas or sufficient bytes), the operation is registered in a structure called Purgatory. This allows Kafka to avoid busy-polling or blocking threads. Instead, the delayed operations are stored and are only \"woken up\" when their specific conditions are met. Purgatory handles these delayed operations until they can be re-submitted to the regular request queue for a handler thread to process and send a response.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response = retriever_chain.invoke({\"input\":\"What is Purgatory?\"})\n",
    "response['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "02981770",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input': 'What is Purgatory?',\n",
       " 'context': [Document(id='6732e4bf-abfa-4c8d-af7b-ef2241b5094e', metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='lets them register interest and be notified only when something changes.How It WorksEnqueueing a Delayed OperationA handler thread examines a request (produce or fetch).If its conditions aren’t met (e.g. not enough replicas, not enough bytes), it creates a DelayedOperation object and registers it with Purgatory under a key (e.g. partition ID).2. Purgatory’s Data StructuresInternally, Kafka maintains a map from keys → sets of delayed ops.There are separate purgatories for produce (ProducerPurgatory) and fetch (FetchPurgatory), but they share the same basic design.3. Triggering ConditionsReplica Ack Events: when a follower replicates an append, the broker’s log manager notifies ProducerPurgatory that “partition P now has N acks.”Log Append Events: when new messages land in the page cache for a partition, the broker notifies FetchPurgatory that “partition P now has new data.”4. Waking Delayed OpsOn each trigger, Purgatory looks up the set of delayed ops for that partition.It tests each'),\n",
       "  Document(id='c89d6f04-2b91-4206-9912-cd60d5debd42', metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='delayed operations that can’t be completed immediately because they’re waiting on some external condition. Rather than busy‐polling (“spin‐wait”) or blocking threads, Kafka registers those operations in Purgatory and wakes them only when their condition is met.There are two primary use-cases where you want to delay a response:Produce Requests with acks=all (or min.insync.replicas > 1)You need to wait until the record is replicated to enough in‐sync replicas before sending the “ACK” back to the producer.2. Fetch Requests with minBytes and maxWaitMsIf the log hasn’t yet accumulated minBytes of data, you either wait up to maxWaitMs or until that much data arrives—whichever comes first.Without Purgatory, the handler threads would have to block or poll in a tight loop, wasting resources. Purgatory lets them register interest and be notified only when something changes.How It WorksEnqueueing a Delayed OperationA handler thread examines a request (produce or fetch).If its conditions aren’t'),\n",
       "  Document(id='8a12e3e0-66af-4d48-a804-8bd143957bb3', metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='and build a request object.Enqueue the parsed request on the central RequestChannelKafka then hands off each parsed request to one of M handler threads. Those threads typically block on RequestChannel.receiveRequest(), do CPU work and disk interaction (log append, fetch data), then build a response object. The response is pushed back onto the RequestChannel so the original processor thread can pick it up and do the socket write.So when a response is ready, the processor registers the channel for write interest, and on the next select() it serializes the response out to the same TCP socket.But what would happen if a produce or fetch request can’t complete immediately? This is where Purgatory comes into picturePurgatoryIn Kafka’s architecture, Purgatory is the in‐memory data structure for delayed operations that can’t be completed immediately because they’re waiting on some external condition. Rather than busy‐polling (“spin‐wait”) or blocking threads, Kafka registers those operations'),\n",
       "  Document(id='603ca448-a41c-47af-8b60-fb42e54927be', metadata={'source': 'https://medium.com/@tirthraj2004/apache-kafka-deep-dive-8f011c6483e1', 'title': 'Apache Kafka Deep Dive. Apache Kafka Internals | by Tirthraj Mahajan | Jul, 2025 | Medium', 'description': 'It prepares the environment, handles optional flags (like -daemon), and then invokes the actual Java class that runs the Kafka server Sets default JVM heap size to 1 GB if not configured. You can…', 'language': 'en'}, page_content='a partition, the broker notifies FetchPurgatory that “partition P now has new data.”4. Waking Delayed OpsOn each trigger, Purgatory looks up the set of delayed ops for that partition.It tests each one’s condition (enoughAcks() or enoughBytes()).Those whose conditions are now satisfied get re-submitted to the regular request queue, so a handler thread can pick them up and send a real response.5. TimeoutsEach delayed op also carries a deadline (e.g. maxWaitMs), so Purgatory will expire it and return a (possibly empty) response if the timeout elapses first.KafkaApache KafkaKafka InternalsScala----Written by Tirthraj Mahajan13 followers·7 followingNo responses yetHelpStatusAboutCareersPressBlogPrivacyRulesTermsText to speech')],\n",
       " 'answer': 'In Kafka\\'s architecture, what happens if a produce or fetch request can’t complete immediately?\\n\\nIn Kafka\\'s architecture, if a produce or fetch request can\\'t complete immediately because it is waiting for some external condition (e.g., enough replicas or sufficient bytes), the operation is registered in a structure called Purgatory. This allows Kafka to avoid busy-polling or blocking threads. Instead, the delayed operations are stored and are only \"woken up\" when their specific conditions are met. Purgatory handles these delayed operations until they can be re-submitted to the regular request queue for a handler thread to process and send a response.'}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a10ca5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RunnableBinding(bound=RunnableAssign(mapper={\n",
       "  context: RunnableBinding(bound=RunnableLambda(lambda x: x['input'])\n",
       "           | VectorStoreRetriever(tags=['FAISS', 'OpenAIEmbeddings'], vectorstore=<langchain_community.vectorstores.faiss.FAISS object at 0x000002081F327F70>, search_kwargs={}), kwargs={}, config={'run_name': 'retrieve_documents'}, config_factories=[])\n",
       "})\n",
       "| RunnableAssign(mapper={\n",
       "    answer: RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
       "              context: RunnableLambda(format_docs)\n",
       "            }), kwargs={}, config={'run_name': 'format_inputs'}, config_factories=[])\n",
       "            | ChatPromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, messages=[HumanMessagePromptTemplate(prompt=PromptTemplate(input_variables=['context'], input_types={}, partial_variables={}, template='\\nAnswer the following question based only on the provided context:\\n<context>\\n{context}\\n</context>\\n\\n'), additional_kwargs={})])\n",
       "            | ChatOpenAI(client=<openai.resources.chat.completions.completions.Completions object at 0x000002081F664A60>, async_client=<openai.resources.chat.completions.completions.AsyncCompletions object at 0x000002081F665480>, root_client=<openai.OpenAI object at 0x000002081F664AF0>, root_async_client=<openai.AsyncOpenAI object at 0x000002081F664B20>, model_name='gpt-4o', model_kwargs={}, openai_api_key=SecretStr('**********'))\n",
       "            | StrOutputParser(), kwargs={}, config={'run_name': 'stuff_documents_chain'}, config_factories=[])\n",
       "  }), kwargs={}, config={'run_name': 'retrieval_chain'}, config_factories=[])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "retriever_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a997f0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
